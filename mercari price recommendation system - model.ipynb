{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"mercari price recommendation system - model.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ftZnGXwm5xVN"},"source":["# run here"]},{"cell_type":"code","metadata":{"id":"XxvRom8t5xVO"},"source":["import time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import nltk\n","\n","from nltk.corpus import stopwords\n","\n","from scipy.sparse import csr_matrix, hstack\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.preprocessing import LabelBinarizer\n","\n","from sklearn.metrics import mean_squared_error\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import KFold, cross_val_score, train_test_split\n","\n","from sklearn.linear_model import Ridge, Lasso, ElasticNet\n","from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n","from lightgbm import LGBMRegressor\n","from xgboost import XGBRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ARfNkCur5xVT"},"source":["# load training data\n","orig_train = pd.read_csv(\"/Users/yf/Documents/big data team project/mercari/train.tsv\", sep = '\\t')\n","orig_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFOZ_Qrx5xVX"},"source":["# drop observations with missing outcome\n","orig_train = orig_train[orig_train.price != 0]\n","orig_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v2qWUroN5xVa"},"source":["# to here"]},{"cell_type":"code","metadata":{"id":"jnwvbED_5xVa"},"source":["# check punctuation\n","from string import punctuation\n","punctuation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bY12uDHh5xVd"},"source":["# check stopwords\n","import nltk\n","#nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","stop_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vvX3GMme5xVg"},"source":["# define functions for text preocessing\n","import string\n","# remove punctuations\n","def remove_punctuation(sentence: str) -> str:\n","    return sentence.translate(str.maketrans('', '', string.punctuation))\n","# remove stop words\n","def remove_stopwords(x):\n","    x = ' '.join([i for i in x.lower().split(' ') if i not in stop_words])\n","    return x\n","# lowercase\n","def to_lower(x):\n","    return x.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7QYLe4sD5xVk"},"source":["# stem the words\n","from nltk.stem.porter import PorterStemmer\n","porter = PorterStemmer()\n","orig_train['item_description'] = orig_train['item_description'].apply(porter.stem)\n","orig_train['item_description'] = orig_train['item_description'].apply(remove_punctuation).apply(remove_stopwords).apply(to_lower)\n","orig_train['name'] = orig_train['name'].apply(remove_punctuation).apply(remove_stopwords).apply(to_lower)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0Ity4xs5xVo"},"source":["# check df again\n","orig_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjCLYNi05xVq"},"source":["# check item description\n","orig_train['item_description'][115:125]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1krSm6P65xVt"},"source":["# tokenize item description\n","#nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","text1 = orig_train['item_description'][120]\n","tokens = word_tokenize(text1)\n","print(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlNkNcGv5xVw"},"source":["# apply countVectorize to name, main category, category, sub category\n","cv = CountVectorizer(min_df=10)\n","x_name = cv.fit_transform(orig_train['name'])\n","x_main_category = cv.fit_transform(orig_train['main_category'])\n","x_category = cv.fit_transform(orig_train['category'])\n","x_sub_category = cv.fit_transform(orig_train['sub_category'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"85ZYgfGt5xVz"},"source":["print(\"Item Name Shape: \" + str(x_name.shape))\n","print(\"Main Category Shape: \" + str(x_main_category.shape))\n","print(\"Category Shape: \" + str(x_category.shape))\n","print(\"Sub Category Shape: \" + str(x_sub_category.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZR_Ki6pY5xV1"},"source":["#### Count Vectorizer Example ####\n","from sklearn.feature_extraction.text import CountVectorizer\n","corpus = [\n","    'This is the first document.',\n","    'This document is the second document.',\n","    'And this is the third one.',\n","    'Is this the first document?',\n","]\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","print(vectorizer.get_feature_names())\n","print(X.toarray())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9esk-Lqa5xV5"},"source":["# apply LabelBinarizer to brand\n","lb = LabelBinarizer(sparse_output=True)\n","x_brand = lb.fit_transform(orig_train['brand_name'])\n","print(\"Item Brand Shape: \" + str(x_brand.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTafuM3i5xV7"},"source":["lb.classes_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U5X6Ibmt5xV-"},"source":["# apply get_dummies to item_condition_id, shipping\n","x_dummies = csr_matrix(pd.get_dummies(orig_train[[\"item_condition_id\",\"shipping\"]],sparse=True).values)\n","print(\"Dummy Shape: \" + str(x_dummies.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcoO-43E5xWA"},"source":["# Perform TFIDF Transformation of the item description \n","# with the top 55000 features and has an n-gram range of 1-2\n","# TfidfVectorizer = CountVectorizer followed by TfidfTransformer\n","tv = TfidfVectorizer(max_features=55000, ngram_range=(1, 2), stop_words='english')\n","x_description = tv.fit_transform(orig_train['item_description'])\n","print(\"Item Description Shape: \" + str(x_description.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3cWuFram5xWD"},"source":["# create a dictionary mapping the tokens to their tfidf values\n","tfidf = dict(zip(tv.get_feature_names(), tv.idf_))\n","tfidf = pd.DataFrame(columns=['tfidf']).from_dict(dict(tfidf), orient='index')\n","tfidf.columns = ['tfidf']\n","# Lowest TFIDF Scores\n","print(tfidf.sort_values(by=['tfidf'], ascending=True).head(10))\n","# Highest TFIDF Scores\n","print(tfidf.sort_values(by=['tfidf'], ascending=False).head(10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-X18virv5xWG"},"source":["# combine everything together\n","# sparse matrix (csr matrix)\n","sparse_merge = hstack((x_dummies, x_description, x_brand, x_name, x_main_category, x_category, x_sub_category)).tocsr()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"myt8Juq75xWJ"},"source":["# run here"]},{"cell_type":"code","metadata":{"id":"ORrB9iiU5xWK"},"source":["# Get 10% of the Training Data\n","train = pd.read_csv(\"/Users/yf/Documents/big data team project/mercari/train.tsv\", sep = '\\t')\n","train = train[train.price != 0]\n","reduced_X_train = train.sample(frac=0.1).reset_index(drop=True)\n","reduced_y_train = np.log1p(reduced_X_train['price'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vvIOzWgQ5xWN"},"source":["# Fast Cleaning of Data\n","reduced_X_train['category_name'] = reduced_X_train['category_name'].fillna('Other').astype(str)\n","reduced_X_train['brand_name'] = reduced_X_train['brand_name'].fillna('missing').astype(str)\n","reduced_X_train['shipping'] = reduced_X_train['shipping'].astype(str)\n","reduced_X_train['item_condition_id'] = reduced_X_train['item_condition_id'].astype(str)\n","reduced_X_train['item_description'] = reduced_X_train['item_description'].fillna('None')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWReei5s5xWQ"},"source":["reduced_X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kZW2-VQv5xWT"},"source":["# to here"]},{"cell_type":"code","metadata":{"id":"tiQocw9Q5xWT"},"source":["%%time\n","# topic modeling + LDA\n","\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","# Initialize CountVectorizer\n","cvectorizer = CountVectorizer(max_features=20000,stop_words='english',lowercase=True)\n","\n","# Fit it to our dataset\n","cvz = cvectorizer.fit_transform(reduced_X_train['item_description'])\n","\n","# Initialize LDA Model with 10 Topics\n","lda_model = LatentDirichletAllocation(n_components=10,random_state=42)\n","\n","# Fit it to our CountVectorizer Transformation\n","X_topics = lda_model.fit_transform(cvz)\n","\n","# Define variables\n","n_top_words = 10\n","topic_summaries = []\n","\n","# Get the topic words\n","topic_word = lda_model.components_\n","\n","# Get the vocabulary from the text features\n","vocab = cvectorizer.get_feature_names()\n","\n","# Display the Topic Models\n","for i, topic_dist in enumerate(topic_word):\n","    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n","    topic_summaries.append(' '.join(topic_words))\n","    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcH9ermy5xWX"},"source":["# define rmsle with cross validation\n","def rmsle_cv(model):\n","    kf = KFold(shuffle=True, random_state=42).get_n_splits(reduced_X_train[\"item_description\"])\n","    rmse = np.sqrt(-cross_val_score(estimator=pipe,X=reduced_X_train[\"item_description\"],y=reduced_y_train,scoring=\"neg_mean_squared_error\",cv=kf))\n","    return (rmse.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2w9PJId85xWb"},"source":["from sklearn.linear_model import Ridge\n","import eli5\n","# baseline model with Count Vectorizer\n","vec = CountVectorizer()\n","clf = Ridge(random_state=42)\n","pipe = make_pipeline(vec, clf)\n","pipe.fit(reduced_X_train[\"item_description\"],reduced_y_train)\n","cv_rmsle = rmsle_cv(pipe)\n","eli5.show_prediction(estimator=clf,vec=vec,doc=reduced_X_train['item_description'][129])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJ71iREq5xWd"},"source":["# baseline model with Count Vectorizer and Stop Words\n","vec = CountVectorizer(stop_words='english')\n","clf = Ridge(random_state=42)\n","pipe = make_pipeline(vec, clf)\n","pipe.fit(reduced_X_train['item_description'], reduced_y_train)\n","cv_sw_rmsle = rmsle_cv(pipe)\n","eli5.show_prediction(estimator=clf,vec=vec,doc=reduced_X_train['item_description'][1297])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eNZnpLs-5xWg"},"source":["# baseline model with TF-IDF\n","vec = TfidfVectorizer()\n","clf = Ridge(random_state=42)\n","pipe = make_pipeline(vec, clf)\n","pipe.fit(reduced_X_train[\"item_description\"],reduced_y_train)\n","tfidf_rmsle = rmsle_cv(pipe)\n","eli5.show_prediction(estimator=clf,vec=vec,doc=reduced_X_train['item_description'][1297])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"It8doFd95xWj"},"source":["# baseline model with TF-IDF and Stop Words\n","vec = TfidfVectorizer(stop_words='english')\n","clf = Ridge(random_state=42)\n","pipe = make_pipeline(vec, clf)\n","pipe.fit(reduced_X_train[\"item_description\"],reduced_y_train)\n","tfidf_sw_rmsle = rmsle_cv(pipe)\n","eli5.show_prediction(estimator=clf,vec=vec,doc=reduced_X_train['item_description'][1297])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zxPHydWm5xWl"},"source":["# baseline model with TF-IDF, Stop Words and N-Gram\n","vec = TfidfVectorizer(stop_words='english',ngram_range=(1,2))\n","clf = Ridge(random_state=42)\n","pipe = make_pipeline(vec, clf)\n","pipe.fit(reduced_X_train[\"item_description\"],reduced_y_train)\n","tfidf_sw_ng_rmsle = rmsle_cv(pipe)\n","eli5.show_prediction(estimator=clf,vec=vec,doc=reduced_X_train['item_description'][1297])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mc06srvs5xWo"},"source":["# RMSLE comparison between models\n","print (\"RMSLE Score: \" + str(cv_rmsle) + \" | CountVectorizer\")\n","print (\"RMSLE Score: \" + str(cv_sw_rmsle) + \" | CountVectorizer | Stop Words\")\n","print (\"RMSLE Score: \" + str(tfidf_rmsle) + \" | TF-IDF\")\n","print (\"RMSLE Score: \" + str(tfidf_sw_rmsle) + \" | TF-IDF | Stop Words\")\n","print (\"RMSLE Score: \" + str(tfidf_sw_ng_rmsle) + \" | TF-IDF | Stop Words | N-Grams\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JGevisJ_5xWr"},"source":["# run here"]},{"cell_type":"code","metadata":{"id":"bc9vGQf05xWs"},"source":["from sklearn.pipeline import FeatureUnion\n","\n","default_preprocessor = CountVectorizer().build_preprocessor()\n","\n","def build_preprocessor(field):\n","    field_idx = list(reduced_X_train.columns).index(field)\n","    return lambda x: default_preprocessor(x[field_idx])\n","\n","vectorizer = FeatureUnion([\n","    ('name', CountVectorizer(\n","        ngram_range=(1, 2),\n","        max_features=50000,\n","        preprocessor=build_preprocessor('name'))),\n","    ('category_name', CountVectorizer(\n","        token_pattern='.+',\n","        preprocessor=build_preprocessor('category_name'))),\n","    ('brand_name', CountVectorizer(\n","        token_pattern='.+',\n","        preprocessor=build_preprocessor('brand_name'))),\n","    ('shipping', CountVectorizer(\n","        token_pattern='\\d+',\n","        preprocessor=build_preprocessor('shipping'))),\n","    ('item_condition_id', CountVectorizer(\n","        token_pattern='\\d+',\n","        preprocessor=build_preprocessor('item_condition_id'))),\n","    ('item_description', TfidfVectorizer(\n","        ngram_range=(1, 2),\n","        max_features=55000,\n","        stop_words='english',\n","        preprocessor=build_preprocessor('item_description'))),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGRes9jM5xWu"},"source":["# Create Transformed Train Set\n","reduced_Xt_train = vectorizer.fit_transform(reduced_X_train.values)\n","reduced_Xt_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6KfMDv1K5xWy"},"source":["# calculate rmsle\n","def get_rmsle(y, pred): return np.sqrt(mean_squared_error(y, pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nLNcN8Bd5xW2"},"source":["%%time\n","# Ridge Cross Validation\n","\n","# Create 3-Fold CV\n","cv = KFold(n_splits=3, shuffle=True, random_state=42)\n","for train_ids, valid_ids in cv.split(reduced_Xt_train):\n","    # Define Ridge Model\n","    model_ridge = Ridge(solver = \"sag\", fit_intercept=True, random_state=42)\n","    \n","    # Fit Ridge Model\n","    model_ridge.fit(reduced_Xt_train[train_ids], reduced_y_train[train_ids])\n","    \n","    # Predict & Evaluate Training Score\n","    y_pred_train = model_ridge.predict(reduced_Xt_train[train_ids])\n","    rmsle_train = get_rmsle(y_pred_train, reduced_y_train[train_ids])\n","    \n","    # Predict & Evaluate Validation Score\n","    y_pred_valid = model_ridge.predict(reduced_Xt_train[valid_ids])\n","    rmsle_valid = get_rmsle(y_pred_valid, reduced_y_train[valid_ids])\n","    \n","    print(f'Ridge Training RMSLE: {rmsle_train:.5f}')\n","    print(f'Ridge Validation RMSLE: {rmsle_valid:.5f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EEBbC_7v5xW5"},"source":["%%time\n","# Lasso Cross Validation\n","\n","# Create 3-Fold CV\n","cv = KFold(n_splits=3, shuffle=True, random_state=42)\n","for train_ids, valid_ids in cv.split(reduced_Xt_train):\n","    # Define Lasso Model\n","    model_lasso = Lasso(fit_intercept=True, random_state=42)\n","    \n","    # Fit Lasso Model\n","    model_lasso.fit(reduced_Xt_train[train_ids], reduced_y_train[train_ids])\n","    \n","    # Predict & Evaluate Training Score\n","    y_pred_train = model_lasso.predict(reduced_Xt_train[train_ids])\n","    rmsle_train = get_rmsle(y_pred_train, reduced_y_train[train_ids])\n","    \n","    # Predict & Evaluate Validation Score\n","    y_pred_valid = model_lasso.predict(reduced_Xt_train[valid_ids])\n","    rmsle_valid = get_rmsle(y_pred_valid, reduced_y_train[valid_ids])\n","    \n","    print(f'Lasso Training RMSLE: {rmsle_train:.5f}')\n","    print(f'Lasso Validation RMSLE: {rmsle_valid:.5f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jj2dfdTk5xW7"},"source":["%%time\n","# ElasticNet Cross Validation\n","\n","# Create 3-Fold CV\n","cv = KFold(n_splits=3, shuffle=True, random_state=42)\n","for train_ids, valid_ids in cv.split(reduced_Xt_train):\n","    # Define ElasticNet Model\n","    model_enet = ElasticNet(random_state=42)\n","    \n","    # Fit ElasticNet Model\n","    model_enet.fit(reduced_Xt_train[train_ids], reduced_y_train[train_ids])\n","    \n","    # Predict & Evaluate Training Score\n","    y_pred_train = model_enet.predict(reduced_Xt_train[train_ids])\n","    rmsle_train = get_rmsle(y_pred_train, reduced_y_train[train_ids])\n","    \n","    # Predict & Evaluate Validation Score\n","    y_pred_valid = model_enet.predict(reduced_Xt_train[valid_ids])\n","    rmsle_valid = get_rmsle(y_pred_valid, reduced_y_train[valid_ids])\n","    \n","    print(f'ElasticNet Training RMSLE: {rmsle_train:.5f}')\n","    print(f'ElasticNet Validation RMSLE: {rmsle_valid:.5f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cSzslXWi5xW-"},"source":["%%time\n","# LightGBM Cross Validation\n","\n","# Create 3-Fold CV\n","cv = KFold(n_splits=3, shuffle=True, random_state=42)\n","for train_ids, valid_ids in cv.split(reduced_Xt_train):\n","    # Define LGBM Model\n","    model_lgb = LGBMRegressor(num_leaves=31, n_jobs=-1, learning_rate=0.1, n_estimators=500, random_state=42)\n","    \n","    # Fit LGBM Model\n","    model_lgb.fit(reduced_Xt_train[train_ids], reduced_y_train[train_ids])\n","    \n","    # Predict & Evaluate Training Score\n","    y_pred_train = model_lgb.predict(reduced_Xt_train[train_ids])\n","    rmsle_train = get_rmsle(y_pred_train, reduced_y_train[train_ids])\n","    \n","    # Predict & Evaluate Validation Score\n","    y_pred_valid = model_lgb.predict(reduced_Xt_train[valid_ids])\n","    rmsle_valid = get_rmsle(y_pred_valid, reduced_y_train[valid_ids])\n","    \n","    print(f'LGBM Training RMSLE: {rmsle_train:.5f}')\n","    print(f'LGBM Validation RMSLE: {rmsle_valid:.5f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5NM3EUFs5xXB"},"source":["# Ensemble step1\n","# Define LGBM Model\n","model_lgb = LGBMRegressor(num_leaves=31, n_jobs=-1, learning_rate=0.1, n_estimators=500, random_state=42)\n","\n","# Fit LGBM Model\n","model_lgb.fit(train_X, train_y)\n","\n","# Predict with LGBM Model\n","lgbm_y_pred = model_lgb.predict(test_X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXvQ_S605xXE"},"source":["# Ensemble step2\n","# Define Ridge Model\n","model_ridge = Ridge(solver = \"lsqr\", fit_intercept=True, random_state=42)\n","    \n","# Fit Ridge Model\n","model_ridge.fit(train_X, train_y)\n","    \n","# Evaluate Training Score\n","ridge_y_pred = model_ridge.predict(test_X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEuPGoJV5xXL"},"source":["# Ensemble step3\n","ensemble_y_pred = (lgbm_y_pred+ridge_y_pred)/2\n","\n","ensemble_rmsle = get_rmsle(ensemble_y_pred, test_y)\n","\n","print(f'Ensemble RMSLE: {ensemble_rmsle:.5f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJkXoeoy5xXR"},"source":["# Prediction\n","ensemble_y = (np.expm1(lgbm_y_pred)+np.expm1(ridge_y_pred))/2\n","ensemble_y[200:220]\n","# Test Predictions \n","np.expm1(test_y[200:220])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mvmi49cw5xXV"},"source":["# to here"]},{"cell_type":"code","metadata":{"id":"1UeAAqxx-z9W"},"source":[""],"execution_count":null,"outputs":[]}]}